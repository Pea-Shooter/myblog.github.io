---
layout: post
title: Dropout详解
date: 2018-03-15 12:00:00.000000000 +09:00
tags: Machine learning  dropout  Deep learning
---

[TOC]

### Motivation of dropout

随着神经网络的日益加深，网络参数越来越多，模型结构越来越复杂，网络从输入到输出能够学习到非常复杂的关系。但是，受限于可用于训练的数据样本，许多复杂的关系可能是采样噪声带来的结果，因此这些关系可能在训练集上表现得很好，但是在测试集上效率却大打折扣，这就是所谓的过拟合现象。

在此之前，有许多方法用于缓解过拟合问题，如当验证集上的结果开始变差的时候就停止训练、应用L1和L2范式进行weight penalty和soft weight sharing等。

在理想情况下（计算资源不受限制），最好的正则化一个确定大小的模型的方法是将所有可能模型的预测结果进行平均。但是在当前可利用的计算资源下，巨大的网络非常缓慢，因此将多个网络进行联结使用的方法在生产中不太现实。

### What is dropout?

基于上述问题，Dropout是用于防止过拟合和提供一种有效近似联结指数级不同神经网络结构的方法。如下图所示，dropout中的drop指随机“丢弃”网络层中的某些节点，一种简单的实现方式是每一个节点都有 $p$ 概率被保留。对一个网络使用dropout相当于从网络中采样一个“变薄”的网络，这个变薄的网络包含所有节点（不管是存活还是被丢弃）。因此，一个有$n$个节点的网络可以看作拥有$2^{n}$个“变薄”的网络的集合，这些网络共享权值，因此总的参数量还是$O(n^{2})$或者更少。对于每一个训练样本，都有一个“薄网络”被采样训练，因此训练一个使用dropout的网络可以看成是在训练权值共享的$2^{n}$个“薄网络”的集合。

![figure1](https://github.com/Pea-Shooter/Pea-Shooter.github.io/raw/master/images/blog/2018-03-15/Figure_1.png)

### How does dropout work?

对于如下网络的训练流程一般是：把输入$x$通过网络前向传播然后把误差反向传播，网络进行学习后输出$y$。

![figure1](https://github.com/Pea-Shooter/Pea-Shooter.github.io/raw/master/images/blog/2018-03-15/Figure_3.png)

在使用dropout之后网络的训练和测试阶段的流程如下：

#### Train

> This part is the main reference of [知乎文章](https://zhuanlan.zhihu.com/p/23178423) 

1. 以 $1-p$ 的概率临时“丢弃”（$p$的概率保留）网络中的隐层神经单元；

![figure1](https://github.com/Pea-Shooter/Pea-Shooter.github.io/raw/master/images/blog/2018-03-15/Figure_4.png)

2. 将输入$x$通过修改后的网络前向传播，得到的损失结果通过修改后的网络反向传播。一小批训练样本执行完该过程后按照随机梯度下降法更新（没有被删除的神经元）对应的参数（$w,b$）；
3. 重复下述过程直到网络收敛：
   * 恢复被“丢弃”的神经元（此时被“丢弃”的神经元的参数保持原样，而没有被“丢弃”的神经元的参数已经有所更新）
   * 以 $1-p$ 的概率临时“丢弃”（$p$的概率保留）网络中的隐层神经单元（备份被“丢弃”神经元的参数）
   * 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（$w,b$），（没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）

#### Test

在测试阶段，显式地将训练中指数级的“薄网络”中求平均是不现实的。实践中的思路是这样：在测试时使用一个不使用dropout的网络，该网络的权值是训练时的网络权值的缩小版，即，如果一个隐层单元在训练过程中以概率p被保留，那么该单元的输出权重在测试时乘以p（如下图所示）。这样共享权值的$2^{n}$个训练网络就可以在测试时近似联结成一个网络，因此能有效降低泛化误差。

![figure1](https://github.com/Pea-Shooter/Pea-Shooter.github.io/raw/master/images/blog/2018-03-15/Figure_2.png)

### Dropout model description

![figure1](https://github.com/Pea-Shooter/Pea-Shooter.github.io/raw/master/images/blog/2018-03-15/Figure_5.png)

考虑一个拥有$L$层隐层的神经网络，$l \in \{1,…,L\}$为隐层的索引，$z^{(l)}$ 表示$l$层 的输入向量，$y^{(l)}$ 表示 $l$ 层的输出（$y^{(0)}=x$ 为输入），$W^{(l)},b^{(l)}$ 分别为 $l$ 层的权值和偏置。标准的神经网络（如上图(a)）的前向传播可以描述为如下（对于$l \in \{0,…,L-1\}$ 和任意隐层单元$i$）：

$$z_{i}^{(l+1)}=W_i^{(l+1)}y^{l}+b_{i}^{(l+1)}$$

$$y_{i}^{(l+1)}=f(z_{i}^{(l+1)}$$

其中$f$ 为任意激活函数，例如$f(x) = 1/(1+exp(-x))$。

dropout网络（上图(b)）的前向传播过程如下：

$$r_{j}^{(l)} \sim Bernoulli(p) $$

$$\breve{y}^{(l)}=r^{(l)}*y^{(l)}$$

$$z_{i}^{(l+1)}=W_{i}^{(l+1)}\breve{y}^{(l)}+b_{i}^{(l+1)}$$

$$y_{i}^{(l+1)}=f(z_{i}^{(l+1)})$$

其中$*$代表element-wise相乘，对任意层$l$，$r^{(l)}$从伯努利分布中采样，其值有$p$概率为1，采样后与该层的输出$y^{(l)}$进行element-wise乘积，产生一个“变薄”的网络层的输出$\breve{y}^{(l)}$，该输出随即用作下一层的输入。该过程应用在网络中的每一层。在test阶段，网络的权值按$W_{test}^{(l)}=pW^{(l)}$比例产生。

### Contribution of dropout

**降低神经元之间复杂的共适应关系**：神经网络（尤其是深度神经网络）在训练过程中，神经元之间会产生复杂的共适应关系，但是我们更希望的是神经元能够自己表达出数据中的共同本质特征。使用dropout后，两个神经元不一定每次都出现在同一个网络中，使得网络中的权值更新不再依赖于具有固定关系的神经元节点之间的共同作用，使得网络更加robust。

**模型平均**：Dropout使得神经网络的训练效果近乎于对$2^{n}$个子网络的平均，有可能使得一些“相反”的拟合互相抵消，从而缓解过拟合的情况。



[**Reference**:Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)