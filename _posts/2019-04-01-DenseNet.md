---
layout: post
title: DenseNet
date: 2019-04-01 21:00:00.000000000 +09:00
tags: Deep Learning
---

[TOC]

> æœ¬æ–‡æ˜¯[Understanding and visualizing DenseNets](<https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a>) ( Pablo Ruiz â€“ Harvard University â€“ August 2018 ) çš„ç¼©ç•¥ç¬”è®°ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥è¯¦ç»†é˜…è¯»è¯¥åšå®¢ã€‚

# 1. Background

Densely Connected Convolutional Networks , DenseNets, are the next step on our way to expand the depth of deep convolutional networks.

We have seen how we have gone from 100 even 100 layers.

# 2. Motivation

Information: input -> output
Gradient: output -> input
With CNN going deeper, they get vanished easier.

> Instead of drawing representational power from extremely deep or wide architectures, DenseNets exploit the potential of the network through feature reuse

# 3. Advantage

1. Each layer has direct access to the gradients from the loss function and the original input image, making it easier to train than those very deep networks. 
2. There is no need to learn redundant feature maps, so it requires fewer parameters
3. DenseNets layers are very narrow, and they just add a small set of new feature-maps

# 4. Architecture

For traditional feed-forward neural networks, 

$$x_{l}=H_{l}\left(x_{l-1}\right)$$

For ResNets,

$$x_{l}=H_{l}\left(x_{l-1}\right)+x_{l-1}$$

here + means sum the output feature maps.

For Densenets,

$$x_{l}=H_{l}\left(\left[x_{0}, x_{1}, \ldots, x_{l-1}\right]\right)$$

DenseNets do not sum the output feature maps of the layer with the incoming feature maps but concatenate them.

Since we are concatenating feature maps, this channel dimension is increasing at every layer. If we make ð» ð‘™ to produce ð‘˜ feature maps every time, then we can generalize for the ð‘™ ð‘¡â„Ž layer:

$$k_{l}=k_{0}+k *(l-1)$$

This hyperparameter k is the **growth rate**. The growth rate regulates how much information is added to the network each layer.

![figure1](https://github.com/Pea-Shooter/Pea-Shooter.github.io/raw/master/images/blog/2019-04-01/1.png)

# 5. Dense and transition block

![figure1](https://github.com/Pea-Shooter/Pea-Shooter.github.io/raw/master/images/blog/2019-04-01/2.png)

> The volume after every Dense Block increase by the growth rate times the number of Dense Layers within that Dense Block

![figure1](https://github.com/Pea-Shooter/Pea-Shooter.github.io/raw/master/images/blog/2019-04-01/3.png)

> * The volume within a Dense Block remains constant
> * The volume and the feature maps are halved after every Transition Block

![figure1](https://github.com/Pea-Shooter/Pea-Shooter.github.io/raw/master/images/blog/2019-04-01/4.png)